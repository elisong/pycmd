# -*- coding: utf-8 -*-
# Description: Google Search
# Usage: gsearch [-h]
# [-s SITE]
# [-f FILE_TYPE]
# [--intitle INTITLE]
# [--inurl INURL]
# [--link LINK]
# [--proxy-host PROXY_HOST]
# [--proxy-port PROXY_PORT]
# [-n NUM]
# [-o OUTPUT_FILE]
# [-t OUTPUT_TITLE]
# query
import argparse
import sys

from bs4 import BeautifulSoup
from googlesearch import get_page, get_random_user_agent, search
from mdutils.mdutils import MdUtils

from .utils import Console


parser = argparse.ArgumentParser(prog="gsearch", description="Google Search")
parser.add_argument("query", type=str, help="query")
parser.add_argument("-s", "--site", type=str, help="restrict to specific site")
parser.add_argument("-f", "--file-type", type=str, help="restrict to specific document type")
parser.add_argument("--intitle", type=str, help="restrict to pages with specific title")
parser.add_argument("--inurl", type=str, help="restrict to pages with specific word in url")
parser.add_argument("--link", type=str, help="restrict to pages that links to specific web")
parser.add_argument("--proxy-host", type=str, help="socks5 proxy host")
parser.add_argument("--proxy-port", type=int, help="socks5 proxy port")
parser.add_argument("-n", "--num", type=int, default=10, help="max number of pages return")
parser.add_argument("-o", "--output-file", type=str, help="output markdown file")
parser.add_argument("-t", "--output-title", type=str, help="output content title")
args = parser.parse_args()


if args.proxy_host and args.proxy_port:
    import socket

    import socks

    socks.set_default_proxy(socks.SOCKS5, args.proxy_host, args.proxy_port)
    socket.socket = socks.socksocket


def get_links():
    query = args.query
    query += f" site:{args.site}" if args.site else ""
    query += f" file_type:{args.file_type}" if args.file_type else ""
    query += f" intitle:{args.intitle}" if args.intitle else ""
    query += f" inurl:{args.inurl}" if args.inurl else ""
    query += f" link:{args.link}" if args.link else ""
    links = []
    for url in search(query, stop=args.num, user_agent=get_random_user_agent()):
        try:
            soup = BeautifulSoup(get_page(url), features="lxml")
            link = "[" + soup.title.string + "](" + url + ")"
            Console.plain(link)
            links.append(link)
        except Exception:
            pass
    return links


def main():
    Console.info("Goole search command:")
    command = " ".join(sys.argv)
    Console.plain(command)
    file_name = args.output_file or args.query
    title = args.output_title or args.query
    mdFile = MdUtils(file_name=file_name, title=title)
    Console.info(f"Ready to write into '{file_name}.md'")
    mdFile.new_paragraph(f"> Content generated by `{command}`")
    links = get_links()
    mdFile.new_list(items=links)
    mdFile.create_md_file()
    Console.info("Saved in {file_name}.md ğŸ¤ ")


if __name__ == "__main__":
    main()
